{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15VjcPPtvwA4Vo2AYRhW4pzIwXEXUDqPl",
      "authorship_tag": "ABX9TyNqbkfl7xT9dcYhedY4JHFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minh-chaudang/IntroAI/blob/main/BiCNN_MI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ywfj1toPxhc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "698e33eb-51a4-40de-8087-558e728c6112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "from nltk.tokenize.punkt import PunktLanguageVars"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import copy"
      ],
      "metadata": {
        "id": "-3AzaUVPjalc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_msrp_corpus(path):\n",
        "    X = []\n",
        "    y = []\n",
        "    with open(path, encoding=\"utf_8_sig\") as f:\n",
        "        # next(f)  # skip header line\n",
        "        for line in f:\n",
        "            cols = line.strip().split(\"\\t\")  # Quality\t#1 ID\t#2 ID\t#1 String\t#2 String\n",
        "            y.append(cols[0])\n",
        "            X.append((cols[3], cols[4]))\n",
        "    return X, np.array(y, dtype=np.int32)"
      ],
      "metadata": {
        "id": "LwIZw4vOx6j3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer(PunktLanguageVars):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Tokenizer, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def tokenize(self, document):\n",
        "        return self.word_tokenize(document)\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self,\n",
        "                 embed_file,\n",
        "                 max_document_length,\n",
        "                 unknown=\"<UNK>\",\n",
        "                 pad=\"<PAD>\",\n",
        "                 tokenizer=None):\n",
        "        self._maxlen = max_document_length\n",
        "        vocabulary, embeddings = self._load_embeddings(embed_file)\n",
        "        embed_size = embeddings.shape[1]\n",
        "        self._unknown = unknown\n",
        "        self._pad = pad\n",
        "        self._vocabulary = vocabulary\n",
        "        self._embeddings = embeddings\n",
        "        self._new_embeddings = []\n",
        "        self._embed_size = embed_size\n",
        "        if unknown not in vocabulary:\n",
        "            self._add_vocabulary(unknown, random=False)\n",
        "        if pad not in vocabulary:\n",
        "            self._add_vocabulary(pad, random=False)\n",
        "        if tokenizer:\n",
        "            self._tokenizer = tokenizer\n",
        "        else:\n",
        "            self._tokenizer = Tokenizer()\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_embeddings(path):\n",
        "        vocabulary = {}\n",
        "        embeddings = []\n",
        "        with open(path) as f:\n",
        "            for line in f:\n",
        "                cols = line.strip().split(\" \")\n",
        "                word = cols[0]\n",
        "                if word not in vocabulary:\n",
        "                    vocabulary[word] = len(vocabulary)\n",
        "                    embeddings.append(np.array(cols[1:], dtype=np.float32))\n",
        "        return vocabulary, np.array(embeddings)\n",
        "\n",
        "    def _add_vocabulary(self, word, random=True):\n",
        "        # if word in self._vocabulary:\n",
        "        #     return\n",
        "        self._vocabulary[word] = len(self._vocabulary)\n",
        "        if random:\n",
        "            word_vector = np.random.uniform(-1, 1, self._embed_size)  # generate a random embedding for an unknown word\n",
        "        else:\n",
        "            word_vector = np.zeros(self._embed_size, dtype=np.float32)\n",
        "        self._new_embeddings.append(word_vector)\n",
        "\n",
        "    def fit(self, raw_documents):\n",
        "        for document in raw_documents:\n",
        "            self._fit_each(document)\n",
        "        return self\n",
        "\n",
        "    def _fit_each(self, raw_document):\n",
        "        for token in self._tokenizer.tokenize(raw_document.lower()):\n",
        "            if token not in self._vocabulary:\n",
        "                self._add_vocabulary(token, random=True)\n",
        "        return self\n",
        "\n",
        "    def transform(self, raw_documents):\n",
        "        samples = []\n",
        "        for document in raw_documents:\n",
        "            samples.append(self._transform_each(document))\n",
        "        return np.array(samples, dtype=np.int32)\n",
        "\n",
        "    def _transform_each(self, raw_document):\n",
        "        tokens = self._tokenizer.tokenize(raw_document.lower())\n",
        "        if len(tokens) > self._maxlen:\n",
        "            print(\"Token length exceeds max_document_length\")\n",
        "            raise\n",
        "        word_ids = np.full(self._maxlen, self._vocabulary[self._pad], dtype=np.int32)\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token in self._vocabulary:\n",
        "                word_ids[i] = self._vocabulary[token]\n",
        "            else:\n",
        "                word_ids[i] = self._vocabulary[self._unknown]\n",
        "        return word_ids\n",
        "\n",
        "    def fit_transform(self, raw_documents):\n",
        "        return self.fit(raw_documents).transform(raw_documents)\n",
        "\n",
        "    def _fit_transform_each(self, raw_document):\n",
        "        return self._fit_each(raw_document)._transform_each(raw_document)\n",
        "\n",
        "    def get_embeddings(self):\n",
        "        if len(self._new_embeddings) > 0:\n",
        "            self._embeddings = np.r_[self._embeddings, self._new_embeddings]\n",
        "            self._new_embeddings = []\n",
        "        return self._embeddings\n",
        "\n",
        "class MsrpCorpusPreprocessor(Preprocessor):\n",
        "\n",
        "    def __init__(self, embed_file):\n",
        "        super(MsrpCorpusPreprocessor, self).__init__(\n",
        "            embed_file=embed_file,\n",
        "            max_document_length=48,\n",
        "            unknown=\"*UNKNOWN*\",\n",
        "            pad=\"*PAD*\",\n",
        "        )\n",
        "    # override\n",
        "    def transform(self, X_raw):\n",
        "        transform_each = super(MsrpCorpusPreprocessor, self)._transform_each\n",
        "        X = []\n",
        "        for X_raw_each in X_raw:\n",
        "            X.append((transform_each(X_raw_each[0]), transform_each(X_raw_each[1])))\n",
        "        return np.array(X)\n",
        "\n",
        "    # override\n",
        "    def fit_transform(self, X_raw):\n",
        "        fit_transform_each = super(MsrpCorpusPreprocessor, self)._fit_transform_each\n",
        "        X = []\n",
        "        for X_raw_each in X_raw:\n",
        "            X.append((fit_transform_each(X_raw_each[0]), fit_transform_each(X_raw_each[1])))\n",
        "        return np.array(X)\n",
        "\n",
        "    def embed_each(self, sentence):\n",
        "        indices = self._transform_each(sentence)\n",
        "        result = []\n",
        "        for index in indices:\n",
        "          result.append(self.embeddings[index])\n",
        "        return np.array(result)\n",
        "\n",
        "    def embed(self, sentence_pairs):\n",
        "      result = []\n",
        "      for pair in sentence_pairs:\n",
        "        result.append([self.embed_each(pair[0]), self.embed_each(pair[1])])\n",
        "\n",
        "      return np.array(result)\n",
        "\n",
        "    @property\n",
        "    def max_sentence_length(self):\n",
        "        return self._maxlen\n",
        "\n",
        "    @property\n",
        "    def embeddings(self):\n",
        "        return self.get_embeddings()"
      ],
      "metadata": {
        "id": "qkWI4mcuyWmR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = MsrpCorpusPreprocessor(\"/content/drive/MyDrive/paraphraseIden/Bi-CNN-MI/sample/embeddings-original.EMBEDDING_SIZE=25.txt\")"
      ],
      "metadata": {
        "id": "GMubaeexRZjD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw,y = load_msrp_corpus(\"/content/drive/MyDrive/paraphraseIden/Bi-CNN-MI/sample/msr_paraphrase_test-small.txt\")\n",
        "X_train = processor.fit_transform(X_raw)"
      ],
      "metadata": {
        "id": "-_mQRi2WRf2r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JrVvg_DT_EGu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(keras.layers.Layer):\n",
        "  def __init__(self, processor):\n",
        "    super().__init__(trainable = False)\n",
        "    self.processor = processor\n",
        "\n",
        "  def call(self, X):\n",
        "    result = []\n",
        "    for S in X: result.append(self.embed_each(S))\n",
        "    return np.array(result)\n",
        "\n",
        "  def embed_each(self, wordIds):\n",
        "    result = []\n",
        "    for id in wordIds: result.append(processor.embeddings[id])\n",
        "    return np.array(result)"
      ],
      "metadata": {
        "id": "-k88MXjh_2qQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Conv2D(filters, window_size):\n",
        "  return keras.layers.Conv2D(filters = filters, kernel_size = (1, window_size), padding = 'same')"
      ],
      "metadata": {
        "id": "qdabjMAgFY5Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Similarity(keras.layers.Layer):\n",
        "  def __init__(self, beta):\n",
        "    self.beta = beta\n",
        "    super().__init__(trainable = False)\n",
        "  def call(self, inputs):\n",
        "    x0, x1 = inputs\n",
        "    temp0 = np.transpose(x0, [0,1,3,2])\n",
        "    temp1 = np.transpose(x1, [0,1,3,2])\n",
        "    nums, channels, words, embed_size = temp0.shape\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for index in range(nums):\n",
        "      for channel in range(channels):\n",
        "        res.append(self.sentence_diff(temp0[index][channel], temp1[index][channel]))  \n",
        "    \n",
        "    return np.array(res).reshape(nums, channels, words, words)\n",
        "\n",
        "  \n",
        "  def word_diff(self, word0, word1):\n",
        "    diff = word0 - word1\n",
        "    distance = np.sum(diff * diff)\n",
        "    return np.exp(- distance / (2 * self.beta))\n",
        "\n",
        "  def sentence_diff(self, sen1, sen2):\n",
        "    res = np.array([[0]*sen2.shape[0] for i in range(sen1.shape[0])])\n",
        "    for i in range(sen1.shape[0]):\n",
        "      for j in range(sen2.shape[0]):\n",
        "        res[i][j] = (self.word_diff(sen1[i], sen2[j]))\n",
        "    return res"
      ],
      "metadata": {
        "id": "nbWNL27aeLmT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiCNN(keras.Model):\n",
        "  def __init__(self, processor, channels, filter_width, k_top, beta, pool_size):\n",
        "    # retain parameters\n",
        "    self.processor = processor\n",
        "    self.filter_width = filter_width\n",
        "    self.channels = channels\n",
        "    self.k_top = k_top\n",
        "    self.beta = beta\n",
        "    self.pool_size = pool_size\n",
        "    super(BiCNN, self).__init__()\n",
        "\n",
        "    self.embed = Embed(processor)\n",
        "    self.conv1l = Conv2D(channels[0], filter_width[0])\n",
        "    self.similarity = Similarity(self.beta)\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    Sl, Sr = x[:, 0], x[:, 1]\n",
        "    El = self.to4d(self.embed(Sl).transpose([0,2,1]))\n",
        "    Er = self.to4d(self.embed(Sr).transpose([0,2,1]))\n",
        "    \n",
        "    El_transposed = np.transpose(El, [0, 3, 1, 2])\n",
        "    Er_transposed = np.transpose(Er, [0, 3, 1, 2])\n",
        "    Fu = self.similarity([El_transposed, Er_transposed])\n",
        "\n",
        "    print(\"Fu\", type(Fu), Fu.shape)\n",
        "\n",
        "    C1l = self.conv1l(El)\n",
        "    C1r = self.conv1l(Er)\n",
        "\n",
        "\n",
        "    C1l_transposed = np.transpose(C1l, [0, 3, 1, 2])\n",
        "    C1r_transposed = np.transpose(C1r, [0, 3, 1, 2])\n",
        "\n",
        "    A1l = self.folding(C1l_transposed)\n",
        "    A1r = self.folding(C1r_transposed)\n",
        "    B1l = np.tanh(A1l)\n",
        "    B1r = np.tanh(A1r)\n",
        "\n",
        "\n",
        "    # print(B1l.shape)\n",
        "\n",
        "    return Fu\n",
        "  \n",
        "  @staticmethod\n",
        "  def to4d(x):\n",
        "    n, h, w = x.shape\n",
        "    return x.reshape((n, h, w, 1))\n",
        "\n",
        "  @staticmethod\n",
        "  def add_row(x):\n",
        "    n, c, w, h = x.shape\n",
        "    return np.concatenate([np.zeros((n, c, 1, h), dtype=x.dtype), x], axis=2)\n",
        "\n",
        "  @staticmethod\n",
        "  def folding(x):\n",
        "    x_odd = x[:, :, 1::2]  # extract odd rows\n",
        "    x_even = x[:, :, ::2]  # extract even rows\n",
        "    d = x_odd.shape[2] - x_even.shape[2]\n",
        "    if d == -1:\n",
        "      x_odd = BiCNN.add_row(x_odd)\n",
        "    elif d == 1:\n",
        "      x_even = BiCNN.add_row(x_even)\n",
        "    return (x_odd + x_even) / 2\n",
        "  \n",
        "  def word_diff(self, word1, word2):\n",
        "    return np.exp(-(np.linalg.norm(word1 - word2)**2)/(2*self.beta))\n",
        "  \n",
        "  def F(self, El, Er):\n",
        "\n",
        "    w1, w2 = El.shape[0], El.shape[0]\n",
        "    result = [[0]*w2 for i in range(w1)]\n",
        "    for i in range(w1):\n",
        "      for j in range(w2):\n",
        "        result[i][j] = self.word_diff(El[i], Er[j])\n",
        "    return np.array(result)\n",
        "  "
      ],
      "metadata": {
        "id": "QixjhXEr28-v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bicnn = BiCNN(processor = processor, channels=[3, 5], filter_width=[6, 14], k_top=4, beta=2, pool_size=[(10, 10), (10, 10), (6, 6), (2, 2)])\n"
      ],
      "metadata": {
        "id": "PbMvllqOATlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers"
      ],
      "metadata": {
        "id": "wP1PyoSrz8hl"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(1,48,48)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0zSm62jzbuc",
        "outputId": "a332f294-5d0e-402d-9ba4-440593f722c4"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_17 (Flatten)        (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 2305      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,305\n",
            "Trainable params: 2,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw,y = load_msrp_corpus(\"/content/drive/MyDrive/paraphraseIden/MSRP/msr_paraphrase_train.txt\")\n",
        "X_train = processor.fit_transform(X_raw)\n",
        "matrices = bicnn(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "refdLw9u5SYz",
        "outputId": "e0997033-4cd3-433a-c8e9-14eeea1b6c12"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fu <class 'numpy.ndarray'> (4076, 1, 48, 48)\n",
            "(4076, 3, 13, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(matrices.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZKcD_DW5AU1",
        "outputId": "f2a7241f-38e5-4d81-e721-5bd1d4a541fa"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4076, 1, 48, 48) (4076,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(matrices, y, epochs=5, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0UH12nSzjAQ",
        "outputId": "4c24a62b-e2d8-4eab-b224-ffc3050203d3"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "115/115 [==============================] - 1s 4ms/step - loss: 0.0000e+00 - accuracy: 0.6731 - val_loss: 0.0000e+00 - val_accuracy: 0.6961\n",
            "Epoch 2/5\n",
            "115/115 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.6731 - val_loss: 0.0000e+00 - val_accuracy: 0.6961\n",
            "Epoch 3/5\n",
            "115/115 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.6731 - val_loss: 0.0000e+00 - val_accuracy: 0.6961\n",
            "Epoch 4/5\n",
            "115/115 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.6731 - val_loss: 0.0000e+00 - val_accuracy: 0.6961\n",
            "Epoch 5/5\n",
            "115/115 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.6731 - val_loss: 0.0000e+00 - val_accuracy: 0.6961\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9b8e4862d0>"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_raw, y_test = load_msrp_corpus(\"/content/drive/MyDrive/paraphraseIden/MSRP/msr_paraphrase_test.txt\")\n",
        "X_test = bicnn(processor.fit_transform(X_test_raw))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOOT3h4C8fjJ",
        "outputId": "d66dc775-7ddb-43af-92c7-feb5daea287b"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fu <class 'numpy.ndarray'> (1725, 1, 48, 48)\n",
            "(1725, 3, 13, 48)\n",
            "Test loss: 0.0\n",
            "Test accuracy: 0.6649275422096252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxQQcnwh9ztW",
        "outputId": "19cdd94f-241d-47d1-de3f-0dabcb88661f"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0\n",
            "Test accuracy: 0.6649275422096252\n"
          ]
        }
      ]
    }
  ]
}